spark-广播变量
将数据发送个各个executor一份，使用的比特协议（人人为我我为人人，把数据分块，如果其中某个executor有块1，需要块1的executor都可以找它要）
实现map join的方式（广播变量。缓存文件(sc.addfile)。闭包）
闭包是每个task都是复制一份

rdd的五大属性
 1.compute()      描述本rdd的数据是怎么被计算出来的
 2.依赖rdd列表     存储本rdd依赖的一个或多个rdd
 3.分区列表        对数据进行切片
 4.[可选]分区器
 5.[可选]每个分区首选的计算执行位置

rdd是什么：弹性分布式数据集，是个数据集但是里面没有数据，想要获取它代表的数据就去调分区器
为什么是懒加载：转换算子就是调用父rdd的迭代器生成一个新的迭代器，只有行动算子才回去遍历迭代器

Dependency依赖关系
用会触发shuffle的算子一定会触发一个宽依赖吗，比如reducebykey
    val rdd4: RDD[(String, Int)] = rdd3.partitionBy(new HashPartitioner(3))
    val rdd5: RDD[(String, Int)] = rdd4.reduceByKey(_ + _, 3)
先根据hash进行分区，再调用reducebykey(默认用hash分区),这样就完全没有必要再触发shuffle,rdd4和rdd5都会在同一份stage中
只有在父rdd和子rdd的分区器和分区数相同，就不会触发shuffle(前提是父rdd没有弄丢分区器，
                                                      比如repartition会调用coalesce，然后调用coalescedRDD,再调用map，map不会保留父rdd的分区器)


默认分区 ：spark.default.parallelism优先级最高
 local模式下：总核数，最低为1
 分布式模式下：总核数，最低为2
 数据源的分区数看数据源读取器
 有默认分区数参数就用参数，没有就用上游rdd的最大分区数

默认分区器策略：
 如果存在拥有分区器的父rdd & （父rdd中的最大分区数/父rdd中的最大分区器的分区数<10倍 || 默认分区数<=最大分区器的分区数）
  就用上游最大分区器作为子RDD的分区器
  否则创建一个hashpartitioner分区器，分区数=默认分区数，其次就是上游rdd最大分区数

task在spark内分两种：shuffleMapTask和resultTask
 最后一个stage的task都是resultTask，其余stage中的task都是shuffleMapTask
 都是调用final迭代器，不过shufflemaptask要进行shuffle，result task负责返回计算结果
 shuffle后的第一个rdd的computer函数是rdd.iter.shufflereader

spark在运行中的各个角色
sparkSubmit：应用提交的客户端程序
Driver:创建并利用sparkcontext进行stage划分，生成taskset，调度task
applicationMaster(cluster模式):yarn的规范中要求的用户主管进程，申请容器启动excutor进程，以及调用driver线程功能
EexcutorLauncher(Client模式):简化后的APPmaster，负责向yarn申请容器来启动excutor进程
Excutor:负责调用task的runtask方法来运行task的逻辑
Task(对象):封装了对FinalRDD的迭代器进行循环调用的逻辑

spark on yarn cluster模式提交流程
1.提交命令后创建一个sparkSubmit进程
2.向resourcemanager申请创建application
3.创建容器
4.返回application id和可用资源
5.将jar包配置文件等上传到hdfs
6.设置容器的env
7.根据env准备容器的环境(刚刚上传到hdfs的文件)
8.发送启动applicationmaster的shell命令
9.applicationmaster启动driver线程，初始化sparkcontext
10.applicationmaster申请运行excutor的容器
11.启动excutor容器，
12.excutor想driver注册自己
13.driver的taskScheduler发送task对象



spark sql的运行原理
1.元数据管理 SessionCatalog
    SessionCatalog主要用于各种函数资源信息和元数据信息(数据库，数据表，数据视图，数据分区和函数等)的统一管理
    创建临时表或者视图，其实是往SessionCatalog注册，进行逻辑计划元数据绑定时，也是从catalog中获取数据
2.SQL解析成逻辑计划
    当调用SparkSession的SQL或者SQLContext的sql方法，就会使用SparkSqlParser进行SQL解析
    主要分两个步骤生成逻辑计划
     1,词法解析：负责将token分组成符号类
     2,语法解析：构建一棵分析树或者抽象语法树
3.元数据绑定逻辑计划
    使用事先定义好的Rule以及SessionCatalog等信息对逻辑计划进行元数据绑定
4.优化逻辑计划
    优化器也是会定义一套Rules，利用这些Rule对逻辑计划和Exeperssion进行迭代处理，从而使树的节点进行合并和优化
    1，谓词下推
    2，列裁剪
    3，常量替换
    4，常量累加
5.使用SparkPlanner生成物理计划
    SparkPlanner使用Planning Strategies，对优化后的逻辑计划进行转换，生成可以执行的物理计划SparkPlan
6.从物理计划获取inputRdd执行
    从物理计划上，获取inputRdd
    从物理计划上，生成全阶段代码，并编译反射出迭代器newBitIterator的clazz[真名：bufferedRowIterator]
    然后将inputRDD做一个transformation得到最终要执行的rdd
